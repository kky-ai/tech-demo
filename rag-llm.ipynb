{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kky-ai/tech-demo/blob/main/rag-llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT API\n",
    "---\n",
    "This way we can call the OpenAI ChatGPT API from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Init (pass your API key)\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "# API Call\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Print the first result (model's answer)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector database + RAG\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kitt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 33/33 [00:00<00:00, 294493.69it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def suck_around(context, idx, maxLen):\n",
    "    words = nltk.tokenize.word_tokenize(context)\n",
    "    if len(words) <= maxLen:\n",
    "        return context\n",
    "\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    cumul = 0\n",
    "    ret = []\n",
    "    for sent_i, sent in enumerate(sentences):\n",
    "        cumul += len(sent)\n",
    "        \n",
    "        if idx < cumul:\n",
    "            # add previous sentence (if any)\n",
    "            if sent_i > 0:\n",
    "                ret.append(sentences[sent_i-1])\n",
    "            \n",
    "            # add current\n",
    "            ret.append(sent)\n",
    "            \n",
    "            # add following (if any)\n",
    "            if sent_i < len(sentences)-1:\n",
    "                ret.append(sentences[sent_i+1])\n",
    "            \n",
    "            # keep adding previous (full) sentences until maxLen not exceeded\n",
    "            tokens_now = nltk.tokenize.word_tokenize(' '.join(ret))\n",
    "            tokens_in_sent = {si: nltk.tokenize.word_tokenize(s) for si, s in enumerate(sentences[:sent_i-1])}\n",
    "            \n",
    "            for si, s in tokens_in_sent.items():\n",
    "                if len(tokens_now) + len(s) <= maxLen:\n",
    "                    ret.insert(0, sentences[si])\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return ' '.join(ret)\n",
    "    \n",
    "MAX_CONTEXT_LEN = 16\n",
    "QPERLINE = 1\n",
    "\n",
    "FN_IN = f'example_data/interview-1.txt'\n",
    "data = []\n",
    "\n",
    "with open(FN_IN, 'r') as fr: \n",
    "    linei = 0\n",
    "    for line in tqdm(fr.readlines()):\n",
    "        linei += 1\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if QPERLINE > 1:\n",
    "                for i in range(QPERLINE):\n",
    "                    idx = int((len(line)/QPERLINE)*i)\n",
    "                    context = suck_around(line, idx, MAX_CONTEXT_LEN)\n",
    "                    data.append({'context': context, 'iline': linei})\n",
    "            else:\n",
    "                data.append({'context': line, 'iline': linei})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitt/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "encoder = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "vectors = encoder.encode([d['context'] for d in data])\n",
    "\n",
    "index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "index.add(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Your searched query\n",
    "query = 'Which schools did you attend?'\n",
    "\n",
    "# Encode the query into a vector\n",
    "xq = encoder.encode([query]) \n",
    "\n",
    "k = 3  ## return k items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == Query: Which schools did you attend?\n",
      " == Searching in a vector database of 17 items => 3 closest:\n",
      "line 21, d = 4.1: And where did you go to college?\n",
      "line 23, d = 9.53: I went to University of California in San Diego, and for my undergrad, and then I went to University of Santa Clara, up in Santa Clara, California. And what did you study? So for my undergrad, it was computer engineering, which was, at that time, it was part computer science and part electrical engineering. And then for my master's degree, I got a master's in electrical engineering.\n",
      "line 3, d = 9.73: Oh, well, I was born in Hollywood, California. And I was born in 1961. And where did you grow up? So I grew up in Southern California. And so half my childhood was in Los Angeles. And then we moved to San Diego, which was great. And, and lived there until I graduated from college.\n",
      "CPU times: user 134 µs, sys: 18 µs, total: 152 µs\n",
      "Wall time: 138 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dists, inds = index.search(xq, k)\n",
    "print(f' == Query: {query}')\n",
    "print(f' == Searching in a vector database of {index.ntotal} items => {k} closest:')\n",
    "for d, i in zip(dists[0], inds[0]):\n",
    "    print(f'line {data[i][\"iline\"]}, d = {round(float(d), 2)}: {data[i][\"context\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG\n",
    "---\n",
    "```faiss``` + ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Init (pass your API key)\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "context = ' <context> '.join([data[i]['context'] for i in inds[0]])\n",
    "\n",
    "# API Call\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a RAG system. I will provide you the context and the query. Answer briefly and correctly.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{context} <query>: {query}\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Print the first result (model's answer)\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
