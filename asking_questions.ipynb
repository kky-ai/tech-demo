{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kky-ai/tech-demo/blob/main/asking_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Asking Questions Model - Example Usage"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Downlaod the AQ model from the GDrive\n",
        "!gdown 1PSaBDLs5z6OF2ej8O_JemzVxNwSaHiic && unzip -u aq.zip\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Install libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers==4.18 sentencepiece"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikKpBRH-qljh"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from random import choice\n",
        "\n",
        "\n",
        "class AQModel:\n",
        "    \"\"\" Asking Questions Model \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.load_tokenizer()\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "        \n",
        "    def load_tokenizer(self):\n",
        "        self.tokenizer = T5Tokenizer('aq/tokenizer/sentencepiece.model')\n",
        "        print(f'Tokenizer loaded.')\n",
        "\n",
        "    def load_model(self):\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained('aq/model', from_tf=True)\n",
        "        print(f'AQ-Model loaded.')\n",
        "\n",
        "    def generate(self, context, choose_from=1):\n",
        "        input_ids = self.tokenizer([context], return_tensors=\"pt\").input_ids\n",
        "        outputs = self.model.generate(input_ids, \n",
        "                                        max_length=128,\n",
        "                                        num_beams=5,\n",
        "                                        no_repeat_ngram_size=2, \n",
        "                                        num_return_sequences=choose_from, \n",
        "                                        early_stopping=True)\n",
        "\n",
        "        decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        \n",
        "        if choose_from > 1:\n",
        "            print(f'Randomly choosing from: ')\n",
        "            for d in decoded:\n",
        "                print(f'{self.parse(d)}')\n",
        "        \n",
        "        return self.parse(choice(decoded))\n",
        "    \n",
        "    def parse(self, generated):\n",
        "        try:\n",
        "            q, a = generated.split('Answer:')\n",
        "            return q.replace('Question:', '').split('?')[0].strip()+'?', a.strip()\n",
        "        except:\n",
        "            print('W: Not parsed.')\n",
        "            return generated\n",
        "        \n",
        "\n",
        "aq = AQModel()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WcHqMGt-qljj",
        "outputId": "6bb11ba1-dc2f-48a9-bebb-8ae921376f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How long did Joe Franklin's program run on local television?\n",
            "A: 43 years\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"\n",
        "    His name is Joe Franklin, and his program ran for 43 years on local \n",
        "    television in New York. And he claims that he invented the talk show format.\n",
        "\"\"\"\n",
        "\n",
        "question, answer = aq.generate(context, choose_from=1)\n",
        "\n",
        "print(f'Q: {question}')\n",
        "print(f'A: {answer}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Sliding window for generating multiple qustions for a given text (txt file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def suck_around(context, idx, maxLen):\n",
        "    words = nltk.tokenize.word_tokenize(context)\n",
        "    if len(words) <= maxLen:\n",
        "        return context\n",
        "\n",
        "    sentences = nltk.sent_tokenize(context)\n",
        "    cumul = 0\n",
        "    ret = []\n",
        "    for sent_i, sent in enumerate(sentences):\n",
        "        cumul += len(sent)\n",
        "        \n",
        "        if idx < cumul:\n",
        "            # add previous sentence (if any)\n",
        "            if sent_i > 0:\n",
        "                ret.append(sentences[sent_i-1])\n",
        "            \n",
        "            # add current\n",
        "            ret.append(sent)\n",
        "            \n",
        "            # add following (if any)\n",
        "            if sent_i < len(sentences)-1:\n",
        "                ret.append(sentences[sent_i+1])\n",
        "            \n",
        "            # keep adding previous (full) sentences until maxLen not exceeded\n",
        "            tokens_now = nltk.tokenize.word_tokenize(' '.join(ret))\n",
        "            tokens_in_sent = {si: nltk.tokenize.word_tokenize(s) for si, s in enumerate(sentences[:sent_i-1])}\n",
        "            \n",
        "            for si, s in tokens_in_sent.items():\n",
        "                if len(tokens_now) + len(s) <= maxLen:\n",
        "                    ret.insert(0, sentences[si])\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            break\n",
        "    \n",
        "    return ' '.join(ret)\n",
        "    \n",
        "MAX_CONTEXT_LEN = 32\n",
        "QPERLINE = 1\n",
        "\n",
        "FN_IN = f'test.txt'\n",
        "FN_OUT = f'context-question-answer.txt'\n",
        "\n",
        "with open(FN_OUT, 'w') as tfw:\n",
        "    with open(FN_IN, 'r') as fr: \n",
        "        for line in tqdm(fr.readlines()):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                if QPERLINE > 1:\n",
        "                    for i in range(QPERLINE):\n",
        "                        idx = int((len(line)/QPERLINE)*i)\n",
        "                        context = suck_around(line, idx, MAX_CONTEXT_LEN)\n",
        "                        question, answer = aq.generate(context, choose_from=1)\n",
        "                        print(context, file=tfw)\n",
        "                        print(question, answer, sep='\\t', file=tfw, end='\\n\\n')\n",
        "                else:\n",
        "                    question, answer = aq.generate(line, choose_from=1)\n",
        "                    print(line, file=tfw)\n",
        "                    print(question, answer, sep='\\t', file=tfw, end='\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trans",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
